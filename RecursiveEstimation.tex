% !TeX program = xelatex
% !TeX encoding = utf8
% !TeX root = RecursiveEstimation.tex
% vim:set ts=2 sw=2 et spell tw=78:

\documentclass[margin=small]{hsrzf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages

%% The OST Student's package
\usepackage{oststud}

% TODO: remove once oststud gets updated
\makeatletter
\let\ost@expectation\relax \let\ost@variance\relax
\let\E\relax \let\Var\relax
\DeclareMathOperator*{\ost@expectation}{E}
\newcommand*{\E}[2][]{\ost@expectation_{#1}\left\{#2\right\}}
\DeclareMathOperator*{\ost@variance}{Var}
\newcommand*{\Var}[2][]{\ost@variance_{#1}\left\{#2\right\}}
\makeatother

%% Mathematics
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algpseudocode}

%% Lists
\usepackage{enumitem}

%% Language configuration
\usepackage{polyglossia}
\setdefaultlanguage{english}
\setotherlanguage[variant=swiss]{german}

%% License configuration
\usepackage[
  type={CC},
  modifier={by-nc-sa},
  version={4.0},
]{doclicense}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metadata

\course{Electrical Engineering}
\module{Recursive Estimation}
\semester{Spring Semester 2023}

\authoremail{npross@student.ethz.ch}
\author{\textsl{Naoki Sean Pross} -- \texttt{\theauthoremail}}

% did someone help you with this work?
\contributors{
  % I created this template, does that count?
  Naoki Pross
  % do not forget to add yourself!
}

\title{Notes of \themodule}
\date{\thesemester}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{alg}{Algorithm}[section]

\theoremstyle{remark}
\newtheorem*{example}{Example}
\newtheorem*{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document

\begin{document}

% use roman numberals for introductiory pages
\pagenumbering{roman}

\maketitle

% \begin{abstract}
% \end{abstract}

% show the names of the people who contributed to this document.
% \section*{Contributors}
% \thecontributors

\section*{License}
\doclicenseThis

\tableofcontents

% actual content
\clearpage
\setcounter{page}{1}
\pagenumbering{arabic}

\twocolumn

\section{Probability Basics Review}

Note that the following definitions will not be rigorous.

\subsection{Random Variables}

\begin{defn}[Random Variable]
  We call $x \in \mathcal{X}$ a random variable (RV) from the set of possible
  outcomes $\mathcal{X}$, with an associated probability density function
  (PDF) $p_x : \mathcal{X} \to \mathbb{R}$ that satisfies
  \begin{itemize}
    \item $p_x(\bar{x}) \geq 0$ for all $\bar{x} \in \mathcal{X}$, and
    \item if $\mathcal{X}$is countable (discrete random variable, DRV)
      \[
        \sum_{\bar{x} \in \mathcal{X}} p_x(\bar{x}) = 1
        \quad\text{or}\quad
        \int_\mathcal{X} p_x(\bar{x}) d\bar{x} =1,
      \]
      in the case of a continuous random variable (CRV) ($\mathcal{X}$ is an
      interval).
  \end{itemize}
\end{defn}

The PDF is then used to define the notion of probability, i.e. the probability
that a discrete RV $x$ takes the value $\bar{x} \in \mathcal{X}$ is
$p_x(\bar{x})$, and is written as $\Pr{x = \bar{x}}$.

For a continuous RV $x$ the probability of any specific value is always 0,
instead we can only can only refer to the RV being in some interval $[a,b]
\subseteq \mathcal{X}$ and we write $\Pr{x \in [a,b]} = \int_a^b p_x(\bar{x})
\,d\bar{x}$.

\begin{defn}[Joint PDF]
  Let $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ be RVs. The joint PDF
  satisfies
  \begin{itemize}
    \item $p_{xy}(\bar{x}, \bar{y}) \geq 0$ for all $x \in\mathcal{X}$ and $y
      \in \mathcal{Y}$,
    \item further
      \[
        \sum_{\bar{x}\in\mathcal{X}}\sum_{\bar{y}\in\mathcal{Y}}
        p_{xy}(\bar{x}, \bar{y}) = 1
        \text{ or }
        \iint\limits_{\mathcal{X}\times\mathcal{Y}} p_{xy}(\bar{x}, \bar{y})
        \,d\bar{x}d\bar{y} = 1
      \]
      for DRVs or CRVs, respectively.
  \end{itemize}
\end{defn}

The interpretation of the joint PDF is that both $x$ equals $\bar{x}$ and $y$
equals $\bar{y}$.

\begin{defn}[Marginalization]
  From a joint PDF of $x$ and $y$ we can redefine the PDF of either variable
  by going through each case of the other:
  \[
    p_x(\bar{x}) = \sum_{\bar{y} \in \mathcal{Y}} p_{xy} (\bar{x}, \bar{y})
    \text{~~or~~}
    p_x(\bar{x}) = \int_\mathcal{Y} p_{xy} (\bar{x}, \bar{y}) \,
    d\bar{y}
  \]
  and similar for $p_y(\bar{y})$.
\end{defn}

\begin{defn}[Conditioning]
  Given the RVs $x$, $y$ and $p_{xy}(\bar{x}, \bar{y})$ define
  \[
    p_{x|y}(\bar{x} | \bar{y}) = \frac{p_{xy}(\bar{x}, \bar{y})}{p_y(\bar{y})}
    \iff
    p_{x|y}(\bar{x} | \bar{y}) p_y(\bar{y}) = p_{xy}(\bar{x}, \bar{y})
  \]
  when $p_y(\bar{y}) \neq 0$.
\end{defn}

The $p_{x|y}(\bar{x} | \bar{y})$ is reads $x$ given $y$ ans is to be
understood the probability of $x$ while keeping $y$ fixed.

The above can generalized for more variables by having a vector of RVs
$(x_1,\ldots, x_N) \in \mathcal{X}^N$. In particular $p : \mathcal{X}^N \to
\mathbb{R}$ is still a scalar. Marginalization still applies
\[
  p(x_1, x_3, \ldots, x_{N-1}) = \sum_{(x_{2}, x_{N}) \in \mathcal{X}^2}
  p(x_1, x_2, \ldots, x_N),
\]
and so does conditioning
\[
  p(x_1, \ldots | x_N) p(x_N) = p(x_1, \ldots x_N).
\]
However, now there can be mixed cases of conditioning.
\begin{prop}[Conditioning]
  Given the RVs $x, y, z$:
  \[
    p_{x|yz}(\bar{x} | \bar{y}, \bar{z}) = 
      \frac{p_{xy|z}(\bar{x},\bar{y} | \bar{z})}{p_{y|x}(\bar{y} | \bar{z})}.
  \]
  This generalizes to more variables.
\end{prop}

For RVs there is a notion if independence, that is, they do not affect each
other.

\begin{defn}[Independence]
  The RVs $x$ and $y$ are said to be independent if $p(x|y) = p(x)$.
\end{defn}

From the definition it follows that $p(x,y) = p(x)p(y)$ and $p(y|x) = p(y)$.

\subsection{Expectation and Moments}

The expectation is to be understood as a statistical average, or as a weighted
sum with the coefficients being the probability.

\begin{defn}[Expectation]
  For a RV $x \in \mathcal{X}$
  \[
    \E[x]{x}
       = \sum_{\bar{x} \in \mathcal{X}} \bar{x} p_x(\bar{x})
       \text{ or }
       \int_\mathcal{X} \bar{x} p_x(\bar{x}) \, d\bar{x}.
  \]
\end{defn}

In the definition above $p_x$ can be replaced with a conditional $p_{x|y}$, to
obtain the conditional expectation
\[
  \E[x|y]{x | \bar{y}} 
    = \int_\mathcal{X} \bar{x} p_{x|y}(\bar{x} | \bar{y}) \, d\bar{x}.
\]

\begin{thm}[Law of the Unconscious Statistician]
  Let $y = g(x) \in \mathcal{Y} = g(\mathcal{X})$ where $x \in \mathcal{X}$ is
  a DRV or CRV. Then
  \[
    \E[y]{y} = \sum_{\bar{x} \in \mathcal{X}} g(\bar{x}) p_x(\bar{x})
      \text{ or }
      \int_\mathcal{X} g(\bar{x}) p_x(\bar{x}) \, d\bar{x},
  \]
  or more compactly $\E[y]{y} = \E[x]{g(x)}$.
\end{thm}

\begin{defn}[Variance]
  For a RV $x \in \mathcal{X}$
  \[
    \Var[x]{x} = \E[x]{(x - \E[x]{x})\mt{(x - \E[x]{x})}}.
  \]
  If $x$ is a vector the resulting matrix is sometimes called covariance.
\end{defn}

\subsection{Sampling Distributions}

Most mathematical libraries offer a function to sample a RV that is uniformly
distributed on $(0,1)$ (in MATLAB \texttt{rand()}). In other words we have a
PDF
\[
  p_u(\bar{u}) = \begin{cases}
    1 & \bar{u} \in (0,1) \\
    0 & \text{otherwise}
  \end{cases}.
\]
We can use $p_u$ to generate samples for any other desired PDF using the
following algorithms.

\begin{alg}[Sample a DRV] \label{alg:sample-drv}
  Given a derided PDF $\hat{p}_x$ for a DRV $x \in \mathcal{X} = \mathbb{Z}$,
  its cumulative distribution function (CDF) is a nondecreasing function
  \[
    \hat{F}_x (\bar{x}) = \sum_{i = -\infty}^{\bar{x}} \hat{p}_x (i),
    = \Pr{x \leq \bar{x}}
  \]
  and has the property that $\hat{F}_x(-\infty) = 0$ and $\hat{F}_x(\infty) =
  1$.

  Let $\bar{u}$ be the samples of $u \sim \mathcal{U}(0,1)$. To find
  a sample $\bar{x}$ of $x$ we solve for a $\bar{x}$ such that
  $\hat{F}_x(\bar{x} -1) < \bar{u}$ and $\bar{u} \leq \hat{F}_x(\bar{x})$.
\end{alg}

\begin{alg}[Sample multiple finite DRV]
  Given a desired joint PDF $\hat{p}_{xy}$ for the scalar DRVs $x \in
  \mathcal{X}$ and $y \in \mathcal{Y}$, where $N_x = |\mathcal{X}|$ and
  $N_y = |\mathcal{Y}|$ are both finite, let $\mathcal{Z} = \{1,2,\ldots,
  N_xN_y\}$. Then define a new $\hat{p}_z$ such that $\hat{p}_z(1) =
  \hat{p}_{xy}(1,1)$, $\hat{p}_z(2) = \hat{p}_{xy}(1,2)$, \dots,
  $\hat{p}_z(N_xN_y) = \hat{p}_{xy}(N_x, N_y)$, and apply algorithm
  \ref{alg:sample-drv} to $\hat{p}_z$.
\end{alg}

If the constraint of having finite sets of outcome is a problem, the following
algorithm also works for infinite sets $\mathcal{X}$ and $\mathcal{Y}$.

\begin{alg}[Sample multiple DRVs] \label{alg:sample-multiple-drvs}
  Given a desired joint PDF $\hat{p}_{xy}$, decompose it into
  $\hat{p}_{x|y}(\bar{x}|\bar{y}) \hat{p}_y(\bar{y})$. Apply algorithm
  \ref{alg:sample-drv} to get a sample $\bar{y}$ for $y$ via
  $\hat{p}_y(\bar{y})$, then with $\bar{y}$ fixed apply algorithm
  \ref{alg:sample-drv} again to get $\bar{x}$ for $x$ via
  $\hat{p}_{x|y}(\bar{x}|\bar{y})$.
\end{alg}

\begin{remark}
  The independence of the uniform number generator between successive calls is
  important. Further, both algorithms were described for 2 variables but they
  both generalize any number of DRVs.
\end{remark}

\begin{alg}[Sample a CRV] \label{alg:sample-crv}
  Given a desired piecewise continuous and bounded PDF $\hat{p}_x$ for a CRV
  $x$, let
  \[
    \hat{F}_x(\bar{x})
      = \int_{-\infty}^{\bar{x}} \hat{p}_x(\lambda) \, d\lambda
      = \Pr{x \leq \bar{x}}
  \]
  be the CDF of $x$. To find a sample of $x$ let $\bar{x}$ be any solution to
  $\bar{u} = \hat{F}_x(\bar{x})$, then $x$ has PDF $p_x = \hat{p}_x$.
\end{alg}

\begin{alg}[Sample multiple CRVs]
  Analogously to algorithm \ref{alg:sample-multiple-drvs} decompose the
  given desired joint PDF into $\hat{p}_{xy}(\bar{x},\bar{y}) =
  \hat{p}_{x|y}(\bar{x}|\bar{y}) \hat{p}_y(\bar{y})$. Then, apply algorithm
  \ref{alg:sample-crv} to get a $\bar{y}$ for $y$ via $\hat{p}_y(\bar{y})$,
  and with $\bar{y}$ fixed apply it again to get a sample $\bar{x}$ of $x$
  with $\hat{p}_{x|y}(\bar{x}|\bar{y})$.
\end{alg}

\subsection{Change of Variables}

When we work with functions of RVs we usually also wish to know the
PDFs of the results.

\begin{prop}[Change of variables for DRVs]
  Let $p_y$ be given for $y \in \mathcal{Y}$ and consider $x = g(y) \in
  \mathcal{X} = g(\mathcal{Y})$. For each $\bar{x} \in \mathcal{X}$ let
  \[
    \mathcal{Y}_{\bar{x}} = \{ \bar{y}_i : \bar{y}_i \in \mathcal{Y},
    g(\bar{y}_i) = \bar{x} \},
  \]
  then
  \[
    p_x(\bar{x}) = 
      \sum_{\bar{y} \in \mathcal{Y}_{\bar{x}}} p_y(\bar{y}) =
      \sum_{\bar{y} \in \mathcal{Y} : g(\bar{y}) = \bar{x}}
        p_y(\bar{y}).
  \]
\end{prop}

\begin{prop}[Change of variables for CRVs]
  Consider a strictly monotonic differentiable continuous function $x = g(y)$,
  then
  \[
    p_x(\bar{x}) = \frac{p_y(\bar{y})}{g'(\bar{y})}
      = \frac{p_y \circ g^{-1}(\bar{x})}{g' \circ g^{-1} (\bar{x})}.
  \]
\end{prop}

\begin{prop}[Multivariate change of variables for CRVs]
  \label{pro:multivariate-change-var}
  Let $g : \mathbb{R}^m \to \mathbb{R}^m, w \mapsto g(w)$, be a map with
  nonsingular Jacobian for all $w$, i.e.
  \[
    \det \frac{\partial g}{\partial w} = \det \begin{bmatrix}
      \partial_{w_1} g_1 & \cdots & \partial_{w_m} g_1 \\
      \vdots & \ddots & \vdots \\
      \partial_{w_1} g_m & \cdots & \partial_{w_m} g_m
    \end{bmatrix}
    \neq 0, \quad \forall w.
  \]
  Further, assume that $z = g(w)$ has a unique solution for $w$ in terms of
  $z$, say $w = h(z)$. Then
  \[
    p_z(\bar{z}) = p_w(h(\bar{z})) \left|
      \det \frac{\partial g}{\partial w}(h(\bar{z})) \right|^{-1}.
  \]
\end{prop}

\subsection{Bayes' Theorem}

\begin{thm}[Bayes' theorem] For the RVs $x$ and $z$
\[
  p(x|z) = p(z|x)\frac{p(x)}{p(z)}.
\]
\end{thm}

\begin{remark}
  The interpretation is as follows: $x$ is the unknown quantity of interest
  (state); $p(x)$ is the prior belief of the state; $z$ is an observation
  related to the state; $p(z|x)$ is, for a given state, what is the
  probability of observing $z$? $p(x|z)$ is the posterior belief, that is the
  observation what is the probability that the state is $x$?
\end{remark}

Bayes' theorem is a systematic way of combining prior beliefs with
observations. Since observing $z$ is usually not enough to directly determine
$x$. That is because usually $\dim z < \dim x$ and with noise $p(z|x)$ that is not
``sharp''.

\begin{prop}[Generalization of Bayes' theorem]
  Suppose there are $N$ (vector or scalar) observations $z_1, \ldots, z_N$.
  Assuming conditional independence, i.e.
  \[
    p(z_1, \ldots, z_N|x) = p(z_1|x) \cdots p(z_N|x),
  \]
  then
  \[
    p(x|z_1, \ldots, z_N) 
      = \frac{p(x) \prod_i p(z_i | x)}{p(z_1, \ldots, z_N)},
  \]
  where the normalization
  \[
    p(z_1, \ldots, z_N) = \sum_{x \in \mathcal{X}} p(x) \prod_i p(z_i | x)
  \]
  by the total probability theorem.
\end{prop}
A possible interpretation for the independence assumption is that a
measurement of the state $x$ is corrupted by noise which is independent at
each time step.

\subsection{Gaussian Random Variables}

For Kalman filter we need to know the properties of Gaussian RVs.

\begin{defn}[Gaussian RV (GRV)]
  The PDF of a Gaussian (normally) distributed $D$-dimensional CRV $y = (y_1,
  \ldots, y_D)$ is
  \[
    p(y) = \frac{1}{\sqrt{(2\pi)^D \det \Sigma}} \exp \left( 
        -\frac{1}{2} \mt{(y - \mu)} \minv{\Sigma} (y - \mu)
      \right),
  \]
  where $\mu \in \mathbb{R}^D$ is the mean vector and $\Sigma \in
  \mathbb{R}^{D\times D}$ and $\Sigma \succ 0$ (is a 
  positive definite matrix) and symmetric ($\mt{\Sigma} = \Sigma$).
\end{defn}

\begin{prop}
  In the special case where $\Sigma$ is a diagonal matrix with entries
  $\sigma^2_i$
  \[
    p(y) = \prod_{i=1}^D \frac{1}{\sqrt{2\pi \sigma^2_i}}
      \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2_i}\right).
  \]
  Hence, the PDF is a product of scalar GRVs, and thus the variables are
  mutually independent (the converse is also true).
\end{prop}

\begin{remark}
  For a time-dependent GRV $y(k)$ we say that it is \emph{spatially}
  independent for a fixed time $k$ if $y_1(k)$, \dots, $y_D(k)$ are
  mutually independent, and \emph{temporally} independent if $y(1)$, \dots,
  $y(k)$ are mutually independent.
\end{remark}

\begin{defn}[Jointly GRVs]
  Two GRVs $x$ and $y$ are said to be jointly Gaussian if the vector RV $(x,
  y)$ is also a GRV.
\end{defn}

\begin{remark}
  If two variables are GRVs this does not imply that they are jointly GRVs.
\end{remark}

\begin{prop} \label{lem:independent-grvs-joint}
  If two GRVs $x \sim \mathcal{N}(\mu_x, \Sigma_x)$ and $y \sim
  \mathcal{N}(\mu_y, \Sigma_y)$ are independent, i.e. $p(x,y) = p(x)p(y)$
  then they are jointly Gaussian.
\end{prop}
\begin{proof}
  We assume hat $p(x, y) = p(x)p(y)$. By computing $p(x)p(y)$ we see that it
  is still Gaussian
  \begin{align*}
    &p(x)p(y)
    \propto \exp \Big(
      -\frac{1}{2} \Big[
        \mt{(x - \mu_x)} \minv{\Sigma}_x (x - \mu_x) \\
        &\qquad\qquad\qquad\qquad
          + \mt{(y - \mu_y)} \minv{\Sigma}_y (x - \mu_y)
      \Big]
    \Big) \\
    &= \exp \Bigg(
      -\frac{1}{2} \mt{\begin{bmatrix}
        x - \mu_x \\
        y - \mu_y 
      \end{bmatrix}}
      \begin{bmatrix}
        \minv{\Sigma}_x & 0 \\
        0 & \minv{\Sigma}_y
      \end{bmatrix}
      \begin{bmatrix}
        x - \mu_x \\
        x - \mu_y
      \end{bmatrix}
    \Bigg).
    \qedhere
  \end{align*}
\end{proof}

\begin{lemma}[Affine transformation of a GRV is a GRV]
  \label{lem:affine-grv}
  Let $y$ be a GRV, $M$ a matrix and $b$ a vector of appropriate size, then $x
  = My + b$ is a GRV.
\end{lemma}

\begin{lemma}[Linear combination of jointly GRVs is a GRV]
  Let $x$ and $y$ be jointly Gaussian GRVs, then $z = M_x x + M_y y$ where
  $M_x$ and $M_y$ are constant matrices of appropriate dimensions, is a GRV.

\end{lemma}

\section{Bayesian Tracking}

\subsection{Problem Statement}

Let $x(k) \in \mathcal{X}$ be a vector valued state at time $k \in
\mathbb{Z}^+$ we wish to estimate. Assume $x$ is a DRV and $\mathcal{X}$ is
finite. Let $z(k)$ be a vector valued measurement which can be a DRV or CRV.

The model for the dynamics of the system and measurements may be nonlinear and
time-varying:
\begin{align*}
  x(k) &= q_{k-1}(x(k-1), v(k-1)), & k&=1,2,\ldots \\
  z(k) &= h_k(x(k), w(k)),
\end{align*}
where $x(0), \{v(\cdot)\},$ and $\{w(\cdot)\}$ are mutually independent with
known PDFs. Any known input to the system is not explicitly included as it can
be embedded into $q_{k-1}$ and $h_k$.

Let $z(1:k)$ denote the set $\{z(1), \ldots, z(k)\}$. The goal is to
\emph{efficiently} compute $p(x(k) | z(1:k))$, that is, the full conditional
probability density function of the state.

\subsection{Recursive Estimator}

Bayesian tracking is a two step recursive algorithm:
\begin{itemize}
  \item \textbf{Prior update}: the state estimate is predicted forward using
    the process model.
  \item \textbf{Measurement update}: the prior is combined with observations /
    measurements.
\end{itemize}

\begin{prop}[Prior Update]
  We can predict the PDF of $x(k)$ based on past measurements $z(1:k-1)$ using
  \begin{gather*}
    p(x(k)|z(1:k-1)) = \\
      \sum_{x(k-1) \in \mathcal{X}}
      \underbrace{p(x(k) | x(k-1))}_{\text{process model}} ~
      \underbrace{p(x(k-1) | z(1:k-1))}_{\text{previous iteration}},
  \end{gather*}
  where $p(x(k) | x(k-1))$ can be computed from $p(v(k-1))$ and
  $q_{k-1}(\cdot, \cdot)$ using changes of variables.
\end{prop}

\begin{remark}
  The change of variables required in the prior update may not be
  straightforward.
\end{remark}

\begin{prop}[Measurement Update]
  We combine the new observation using Bayes' rule and get
  \begin{align*}
    p(x(k) &| z(1:k)) = p(x(k) | z(k), z(1:k-1)) = \\
      &\frac{
        \overbrace{p(z(k) | x(k), z(1:k-1))}^{\text{measurement model}}
        \overbrace{p(x(k) | z(1:k-1))}^{\text{prior}}
      }{\underbrace{p(z(k) | z(1:k-1))}_\text{normalization}}
  \end{align*}
  where the normalization can be computed using the total probability theorem
  \begin{align*}
    p(z(k) &| z(1:k-1)) =  \\
      &\sum_{x(k) \in \mathcal{X}} p(z(k) | x(k)) p(x(k) | z(1: k-1)).
  \end{align*}
\end{prop}

To implement this on a computer, there is the following algorithm.

\begin{alg}[Recursive Estimator]
  Enumerate the state space $\mathcal{X} = \{0,1, \ldots, N-1\}$. Define:
  \begin{itemize}
    \item $\vec{a}^i_{k|k} = p_{x(k) | z(1:k)}(i | \bar{z}(1:k))$ with
      $i=0,1$, \dots, $N-1$, an array with $N$ elements used to store the
      posterior PDF at time $k$. 
    \item $\vec{a}^i_{k|k-1} = p_{x(k) | z(1:k-1)}(i | \bar{z}(1:k-1))$,
      $i=0$, \dots, $N-1$ to store the prior PDF at time $k$.
  \end{itemize}
  Then, compute an expression for
  \begin{itemize}
    \item $p_{x(k)|x(k-1)}(i|j)$ using the process model $x(k) =
      q_{k-1}(x(k-1), v(k-1))$ and $p_{v(k-1)}(\bar{v}(k-1))$.
    \item $p_{z(k)|x(k)}(\bar{z}(k)|i)$ using the observation model $z(k) =
      h_k(x(k),w(k))$ and $p_{w(k)}(\bar{w}(k))$.
  \end{itemize}
  \algrenewcomment[1]{\(\triangleright\) {\itshape #1}}
  \begin{algorithmic}
    \Procedure{Estimator}{$p_{x(0)}(0), \ldots, p_{x(0)}(N-1)$}
      \State\Comment{Initialization}
      \State \(\vec{a}^i_{0|0} \gets p_{x(0)}(i), \quad 
          \forall i \in \{0,\ldots,N-1\}
        \)
      \Loop
        \ForAll {$i \in \{0,\ldots,N-1\}$}
          \State\Comment{Prior update}
          \State \(
              \displaystyle \vec{a}^i_{k|k-1} \gets 
              \sum_{j=0}^{N-1} p_{x(k)|x(k-1)}(i | j) ~ \vec{a}^j_{k-1|k-1}
            \)
          \State\Comment{Measurement update}
          \State \(
              \displaystyle \vec{a}^i_{k|k} \gets 
              \frac{p_{z(k)|x(k)}(\bar{z}(k)|i) ~ \vec{a}^j_{k|k-1}}
              {\displaystyle \sum_{j=0}^{N-1} 
                p_{z(k)|x(k)}(\bar{z}(k)|j) ~ \vec{a}^j_{k|k-1}}
            \)
        \EndFor
      \EndLoop
    \EndProcedure
  \end{algorithmic}
\end{alg}

%% TODO: Add example problem?
%% TODO: remarks on choice of parameters / possible errors

\section{Extracting Estimates}

The conditional PDF $p_{x|z}(\bar{x}|\bar{z})$ for the quantity $x$ and its
observations $z$ captures the full information that one has about $x$ in the
Bayesian sense. However, we may be interest in just on an estimate $\hat{x}$
of $x$.

\subsection{Maximum Likelihood (ML)}

ML applies when $x \in \mathcal{X}$ is an unknown (constant) parameter without
a (known) probabilistic description $p_x(\bar{x})$.

\begin{defn}[Maximum Likelihood Estimator]
  Let $z \in \mathcal{Z}$ be the measurement of the observation model
  $p_{z|x}(\bar{z} | \bar{x})$. For a given observation $\bar{z}$ ML seeks the
  value for the parameter $x$ that makes the observation $\bar{z}$ most
  likely:
  \[
    \hat{x}^\text{ML} = \argmax_{\bar{x} \in \mathcal{X}}
      p_{z|x} (\bar{z} | \bar{x}).
  \]
  In this context $p_{z|x} (\bar{z} | \bar{x})$ is called \emph{likelihood
  function}.
\end{defn}

\begin{example}[Least Squares]
  Suppose there is an observation model, whereby we have $m$ measurements of
  $n$ states:
  \[
    \begin{bmatrix} z_1 \\ \vdots \\ z_m \end{bmatrix}
    = \begin{bmatrix}
        h_{11} & \cdots & h_{1n} \\
        \vdots & \ddots & \vdots \\
        h_{m1} & \cdots & h_{mn}
    \end{bmatrix}
    \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
    + \begin{bmatrix} w_1 \\ \vdots \\ w_m \end{bmatrix}.
  \]
  Or, more compactly $z = Hx + w$, with $z,w \in \mathbb{R}^m$, $x \in
  \mathbb{R}^n$, $H \in \mathbb{R}^{m\times n}$, $m > n$ and $w_i \sim
  \mathcal{N}(0,1)$. Further, we assume that the $w_i$ are mutually
  independent and that $H$ has full column rank ($Hx = 0$ implies $x=0$).

  We apply the multivariate change of variables of proposition
  \ref{pro:multivariate-change-var}, so in this case, $z = g_x(w)$ and the
  unique solution is $w = h_x(z) = z - Hx$. Here, the Jacobian is
  nonsingular since
  \[
    \det \frac{\partial g_x}{\partial w} = 
    \det \partial_w \left(Hx + w\right) = 1.
  \]
  Hence
  \[
    p_{z|x}(\bar{z} | \bar{x})
      = p_w(\bar{z} - H\bar{x})
      = \prod_{i=1}^m p_w(\bar{z}_i - H_i\bar{x}),
  \]
  where in the first equality the conditioning of $x$ can be dropped because
  $w$ and $x$ are assumed to be independent, then the second equality is
  justified by the mutual independence of the $w_i$'s, and we use $H_i$ to
  denote the $i$-th row of $H$ (measurement).
  Now, because $p_w$ is Gaussian
  \[
      \prod_{i=1}^m p_w(\bar{z}_i - H_i\bar{x})
      \propto \exp \left(
        -\frac{1}{2} \sum_{i=1}^m (\bar{z}_i - H_i \bar{x})^2
      \right).
  \]
  To apply ML we need to maximixe the sum in the exponent. Differentiating it
  and setting it to zero yields
  \[
    \mt{H}(\bar{z} - H\bar{x}) = 0 \implies
      \bar{x} = \minv{(\mt{H} H)} \mt{H} \bar{z},
  \]
  that is, the least squares solution.
\end{example}

%% TODO: examples from problem set 3

\subsection{Maximum a Posteriori (MAP)}

The maximum a posteriori estimate applies when $x$ is a RV with known PDF.

\begin{defn}[Maximum a Posteriori Estimator]
  Let $z$ be the measurement of tbe observation model $p_{z|x}(\bar{z} |
  \bar{x})$. For a given obserservation $\bar{z}$ MAP looks for the most
  likely choice of $\bar{x}$ given the previous belief about $x$:
  \[
    \hat{x}^\text{MAP} = \argmax_{\bar{x} \in \mathcal{X}}
      p_{z|x}(\bar{z} | \bar{x}) p_x(\bar{x}).
  \]
\end{defn}

\begin{remark}
  If $p_x$ is constant (uniform), then $\hat{x}^\text{MAP} =
  \hat{x}^\text{ML}$, i.e. the prior does not provide any additional
  information.
\end{remark}

\begin{example}
  Consider the scalar observation model $z = x + w$ with $w \sim
  \mathcal{N}(0,1)$, $x \sim \mathcal{N}(\mu, \sigma^2)$, $w$ and $x$
  independent. Then
  \begin{gather*}
    p_x(\bar{x}) \propto \exp\left(
      -\frac{1}{2} \frac{(\bar{x} - \mu)^2}{\sigma^2}
    \right),
    \text{ and } \\
    p_{z|x}(\bar{z}|\bar{x}) \propto \exp\left(
      -\frac{1}{2} (\bar{z} - \bar{x})^2
    \right).
  \end{gather*}
  To apply MAP compute $p_{x|z} (\bar{x} | \bar{z})$, differentiate w.r.t.
  $\bar{x}$ and set to zero. The result is
  \[
    \bar{x} = \frac{1}{1 + \sigma^2} \mu + \frac{\sigma^2}{1 + \sigma^2}
    \bar{z},
  \]
  a weighted sum. Notice that when $\sigma \to \infty$ ($x$ is uniformly
  distributed), $\bar{x} = \bar{z}$ is the ML.
\end{example}

\subsection{Recursive Least Squares (RLS)}

\subsubsection{Problem Statement}

We consider the observation model for a constant $x$
\[
  z(k) = H(k) x + w(k),
  \quad
  z(k), w(k) \in\mathbb{R}^m, x \in\mathbb{R}^n.
\]
The given prior knowledge are the mean $\hat{x}_0 = \E{x}$ and variance $P_x =
\Var{x}$ of $x$. Further, we assume that the noise $w(k)$ is zero-mean
$\E{w(k)} = 0$, with known variance $R(k) = \Var{w(k))}$ and mutually
independent in time $k$ and also with $x$. Typically it is also the case that
we have more equations than observations, i.e. $n > m$, and information is
obtained ``over time''.

The goal is to compute an estimate $\hat{x}(k)$ of $x$ from the observations
$\{\bar{z}(1), \bar{z}(2), \ldots, \bar{z}(k)\}$ in the least square sense,
that is, minimizing a quadratic error.

\subsubsection{Standard Weighted LS}

%% TODO: Add with prior from problem set 3 p 9, and give more context from p3
A naive strategy is to wait until one has collected enough samples to perform
a weighted LS. Neglecting the prior, if we have data until time $k$:
\[
  \begin{bmatrix} z(1) \\ \vdots \\ z(k) \end{bmatrix} =
  \begin{bmatrix} H(1) \\ \vdots \\ H(k) \end{bmatrix} x +
  \begin{bmatrix} w(1) \\ \vdots \\ w(k) \end{bmatrix},
\]
or $\vec{z} = \mx{H} x + \vec{w}$, then $\mx{R} =
\operatorname{blockdiag}(R(1), \ldots, R(k))$
\begin{align*}
  \hat{x}^\text{WLS}(k) &= \argmin_{\hat{x}} \{
    (\mt{\bar{\vec{z}} - \mx{H} \hat{x})} \minv{\mx{R}}
      (\bar{\vec{z}} - \mx{H} \hat{\vec{x}})
  \} \\
  &= \minv{(\mt{\mx{H}} \minv{\mx{R}} \mx{H})}
    \mt{\mx{H}} \minv{\mx{R}} \bar{\vec{z}}
\end{align*}

The obvious problem with this approach is that as time progresses ($k$
increases) the LS problem grows in dimension, until it becomes computationally
intractable.

\subsubsection{Recursive LS}

A more effective solution is a recursive algorithm. This is a precursor to the
Kalman filter.

\begin{alg}[Recursive LS]
  \label{alg:recursive-ls}
  Initialize an estimate of the initial state $\hat{x}_0$ with $P_0
  = P_x = \Var{x}$. The recursion is as follows:
  Observe $\bar{z}_k$, then update
  \begin{align*}
    K_k &= P_{k-1} \mt{H}_k \minv{(H_k P_{k-1}\mt{H}_k + R_k)}, \\
    \hat{x}_k &= \hat{x}_{k-1} + K_k (\bar{z}_k - H_k \hat{x}_{k-1}), \\
    P_k &= (I - K_k H_k) P_{k-1} \mt{(I - K_k H_k)} + K_k R_k \mt{K}_k.
  \end{align*}
\end{alg}

An intuition for the structure of this estimator is given by the fact that if
the measurement coincides with the estimate then $\bar{z}_k - H_k \hat{x}_{k-1}
= 0$ and $\hat{x}_k = \hat{x}_{k-1}$.

\section{Kalman Filter}

The Kalman filter (KF) is a Bayesian estimator for linear time-varying (LTV)
systems with Gaussian process and measurement noise. The KF is particularly
exceptional because it has a closed form analytical solution.

\subsection{Problem Statement}

Consider a LTV system
\begin{align*}
  x(k) &= A(k-1) x(k-1) + u(k-1) + v(k-1), \\
  z(k) &= H(k) x(k) + w(k),
\end{align*}
where $x(k)$ is the state, $u(k)$ is a known control input, $v(k) \sim
\mathcal{N}(0, Q(k))$ is the process noise, $z(k)$ is the measurement and
$w(k) \sim \mathcal{N}(0, R(k))$ is the sensor noise. Further the initial
state $x(0) \sim \mathcal{N}(x_0, P_0)$ and $x(0), \{w(\cdot)\}$ and
$\{w(\cdot)\}$ are mutually independent.

\begin{remark}
  If $v(k)$ has nonzero mean, say $v(k) \sim \mathcal{N}(\alpha, Q(k))$ define
  $\bar{u} = u - \alpha$. Similarly if $w(k) \sim \mathcal{N}(\beta, R(k))$,
  redefine $\bar{z} = z - \beta$.
\end{remark}

\subsection{Bayesian Formulation}

For the Bayesian interpretation of the KF we reformulate the problem using
auxiliary variables ``p'' for the prediction, and ``m'' for measurement:
\begin{align*}
  x_\mathrm{m}(0) &= x(0) \\
  x_\mathrm{p}(k) &= A(k-1) x_\mathrm{m}(k-1) + u(k-1) + v(k-1) \\
  z_\mathrm{m}(k) &= H(k) x_\mathrm{p}(k) + w(k)
\end{align*}
where $x_\mathrm{m}(k)$ is defined via its PDF
\[
  p_{x_\text{m}(k)}(\xi) = p_{x_\text{p}(k)|z_\text{m}(k)}(\xi | \bar{z}(k)),
  \quad\forall \xi.
\]

\begin{lemma}
  With the above formulation
  \begin{align*}
    p_{x_\mathrm{p}(k)}(\xi) &= p_{x(k)|z(1:k-1)}(\xi | \bar{z}(1:k-1)), \\
    p_{x_\mathrm{m}(k)}(\xi) &= p_{x(k)|z(1:k)}(\xi | \bar{z}(1:k)),
  \end{align*}
  for all $\xi$ and $k = 1,2,\ldots$. That is, $x_\mathrm{p}(k)$ is the RV
  $x(k)$ conditioned on $z(1:k-1)$ and $x_\mathrm{m}(k)$ is the RV $x(k)$
  conditioned on $z(1:k)$.
\end{lemma}
% \begin{proof}[Proof sketch]
% TODO: do the proof
% \end{proof}

Now, introducing the following notation for the mean and variance of the
prediction and measurement
\begin{align*}
  \hat{x}_\mathrm{p}(k) &= \E{x_\mathrm{p}(k)}, &
  P_\mathrm{p}(k) &= \Var{x_\mathrm{p}(k)}, \\
  \hat{x}_\mathrm{m}(k) &= \E{x_\mathrm{m}(k)}, &
  P_\mathrm{m}(k) &= \Var{x_\mathrm{m}(k)},
\end{align*}
we make use of the following fact:
\begin{lemma}
  For all $k$, $x_\mathrm{p}(k)$ and $x_\mathrm{m}(k)$ are GRVs.
\end{lemma}
% \begin{proof}[Proof sketch]
%   The proof is by induction. The claim is true by definition for
%   $x_\mathrm{m}(0)$. For $k \geq 1$ assume $x_\mathrm{m}(k-1)$ is a GRV to
%   show that this implies that $x_\mathrm{p}(k)$ and $x_\mathrm{m}(k)$ are
%   GRVs. This is accomplished by noting that $x_\mathrm{p}(k)$ and
%   $x_\mathrm{m}(k)$ are affine combinations of GRVs and making use of lemma
%   \ref{lem:affine-grv}. Further, $x_\mathrm{m}(k-1)$ and $v(k-1)$ are
%   independent, hence they are also jointly Gaussian by lemma
%   \ref{lem:independent-grvs-joint}.
% TODO: finish here
% \end{proof}

Hence, we can compute expressions for $\hat{x}_\mathrm{p}(k)$,
$P_\mathrm{p}(k)$, $\hat{x}_\mathrm{m}(k)$ and $P_\mathrm{m}(k)$, i.e. the
Kalman filter equations.

\begin{thm}[Kalman Filter Equations]
  The prior update or prediction step is
  \begin{align*}
    \hat{x}_\mathrm{p}(k) &= A(k-1) \hat{x}_\mathrm{m}(k-1) + u(k-1), \\
    P_\mathrm{p}(k) &= A(k-1) P_\mathrm{m}(k-1) \mt{A}(k-1) + Q(k-1),
  \end{align*}
  then, the a posteriori update or measurement step is
  \begin{align*}
    P_\mathrm{m}(k) &= \minv{(
        \minv{P}_\mathrm{p}(k) + \mt{H}(k) \minv{R}(k) H(k)
      )}, \\
    \hat{x}_\mathrm{m}(k) &= \hat{x}_\mathrm{p}(k)
      + P_\mathrm{m}(k) \mt{H}(k) \minv{R}(k)
        (\bar{z}(k) - H(k) \hat{x}_\mathrm{p}(k)).
  \end{align*}
\end{thm}

Therefore, the Kalman filter is the analytical solution to the Bayesian state
estimation problem for a linear system with Gaussian distributions.

\subsection{Alternate Formulation}

A more common formulation for the Kalman filter is as follows.

\begin{thm}[KF with Kalman Gain]
  Let
  \[
    K(k) = P_\mathrm{p}(k) \mt{H}(k) \minv{(
      H(k) P_\mathrm{p}(k) \mt{H}(k) + R(k)
    )}
  \]
  be the Kalman filter gain. Then, the a posteriori update can be computed
  with
  \begin{align*}
    \hat{x}_\mathrm{m}(k) &= \hat{x}_\mathrm{p}(k) + K(k)(
      \bar{z}(k) - H(k) \hat{x}_\mathrm{p}(k)
    ), \\
    P_\mathrm{m}(k) &= (I - K(k) H(k)) P_\mathrm{p}(k).
  \end{align*}
\end{thm}
% \begin{proof}[Proof sketch]
% TODO: do it
% \end{proof}

This is the same as the recursive least square algorithm
\ref{alg:recursive-ls}, therefore, the KF can also be applied to non-Gaussian
RVs. Then, the KF can be interpreted as a linear unbiased estimator that
minimizes the mean square error (MMSE). However, it the KF will no longer be
optimal in the Bayesian sense.

\begin{lemma}[Joseph form of the covariance update]
  In the a posteriori update of the KF with Kalman Gain $P_\mathrm{m}(k)$ can
  be computed with
  \begin{align*}
    P_\mathrm{m}(k) = (I - K(k) H(k)) 
      &P_\mathrm{p}(k) \mt{(I - K(k) H(k))} \\
      &+ K(k) R(k) \mt{K(k)}.
  \end{align*}
  This form is more computationally expensive, but less sensitive to numerical
  errors.
\end{lemma}

\subsection{Implementation of the KF}

\subsection{KF as State Observer}

\end{document}
